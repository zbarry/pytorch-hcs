{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the performance of a model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import holoviews as hv\n",
    "import hvplot.pandas\n",
    "import janitor\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from pytorch_hcs.datasets import BBBC021DataModule\n",
    "from pytorch_hcs.models import ResNet18, ResNet101, ResNet18Embeddings\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "hv.extension('bokeh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path(\"data\")\n",
    "data_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choose GPU or CPU processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\"\n",
    "# DEVICE = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize W&B run\n",
    "\n",
    "This is only so we can load model checkpoints from W&B artifacts.\n",
    "Future TODO would be to store the results of the evaluation run in W&B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login()\n",
    "run = wandb.init(project='pytorch-hcs', name='evaluation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specify model to load\n",
    "\n",
    "Model checkpoint artifacts from the training will be accessible under `'model-{version}'`,\n",
    "where `version` is by default set to the class name of the PyTorch-Lightning module.\n",
    "You can find your model artifact names as a tab in the left pane under the W&B run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_project = 'zbarry/pytorch-hcs'\n",
    "\n",
    "model_id, model_cls = \"resnet18:latest\", ResNet18\n",
    "# model_id, model_cls = \"resnet101:latest\", ResNet101\n",
    "# model_id, model_cls = \"resnet18-embeddings:latest\", ResNet18Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model\n",
    "\n",
    "Download model .ckpt file from W&B. \n",
    "Note that a model `.ckpt` file can be loaded directly through `.load_from_checkpoint` in the `data/weights` directory\n",
    "rather than downloaded from W&B, if desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artifact = run.use_artifact(\n",
    "    f\"{user_project}/{model_id}\", type=\"model\"\n",
    ")\n",
    "\n",
    "artifact_dir = artifact.download()\n",
    "\n",
    "ckpt_path = f\"{artifact_dir}/model.ckpt\"\n",
    "\n",
    "model = model_cls.load_from_checkpoint(str(ckpt_path)).eval().to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up `LightningDataModule`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = BBBC021DataModule(\n",
    "    num_workers=8 if DEVICE != \"cpu\" else 0,  # h5py pickling error otherwise...\n",
    "    tv_batch_size=16,\n",
    "    t_batch_size=8,\n",
    ")\n",
    "\n",
    "dm.setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterate through dataset, extracting predicted class labels from model\n",
    "\n",
    "Training and validation datasets also available for exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = dm.train_dataset\n",
    "# dataloader = dm.train_dataloader()\n",
    "\n",
    "# dataset = dm.val_dataset\n",
    "# dataloader = dm.val_dataloader()\n",
    "\n",
    "dataset = dm.test_dataset\n",
    "dataloader = dm.test_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels = []\n",
    "predicted_labels = []\n",
    "image_idcs = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for image_batch, true_labels_batch, metadata_batch in tqdm(dataloader):\n",
    "        image_idcs.extend(metadata_batch.image_idx.numpy())\n",
    "        outputs = model(image_batch.to(DEVICE))\n",
    "\n",
    "        labels = torch.argmax(outputs, 1).cpu()\n",
    "\n",
    "        predicted_labels.extend(labels)\n",
    "\n",
    "        true_labels.extend(true_labels_batch)\n",
    "\n",
    "true_labels = np.array(true_labels)\n",
    "predicted_labels = np.array(predicted_labels)\n",
    "image_idcs = np.array(image_idcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Matthews correlation coefficient\n",
    "\n",
    "See the [Wikipedia entry](https://en.wikipedia.org/wiki/Phi_coefficient).\n",
    "MCC is useful for a multiclass classification problem with highly imbalanced classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import matthews_corrcoef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcc = matthews_corrcoef(true_labels, predicted_labels)\n",
    "mcc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Display images which were predicted incorrectly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_image_idcs = image_idcs[np.flatnonzero(predicted_labels != true_labels)]\n",
    "error_image_idcs\n",
    "\n",
    "error_predictions = predicted_labels[np.flatnonzero(predicted_labels != true_labels)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pybbbc import BBBC021\n",
    "\n",
    "bbbc021 = BBBC021()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_layout(image_idx):\n",
    "    image, metadata = bbbc021[error_image_idcs[image_idx]]\n",
    "\n",
    "    predicted_class = dataset.label_to_class[error_predictions[image_idx]]\n",
    "\n",
    "    prefix = f\"{metadata.compound.compound}, {metadata.compound.moa}, Pred: {predicted_class}\"\n",
    "\n",
    "    plots = []\n",
    "\n",
    "    cmaps = [\"fire\", \"kg\", \"kb\"]\n",
    "\n",
    "    for channel_idx, im_channel in enumerate(image):\n",
    "        plot = hv.Image(\n",
    "            im_channel,\n",
    "            bounds=(0, 0, im_channel.shape[1], im_channel.shape[0]),\n",
    "            label=f\"{prefix}\",  # | {bbbc021.CHANNELS[channel_idx]}\",\n",
    "        ).opts(cmap=cmaps[channel_idx])\n",
    "        plots.append(plot)\n",
    "\n",
    "    plots.append(\n",
    "        hv.RGB(\n",
    "            image.transpose(1, 2, 0),\n",
    "            bounds=(0, 0, im_channel.shape[1], im_channel.shape[0]),\n",
    "            label=\"Channel overlay\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return hv.Layout(plots).cols(2)\n",
    "\n",
    "\n",
    "hv.DynamicMap(make_layout, kdims=\"image\").redim.range(\n",
    "    image=(0, len(error_image_idcs) - 1)\n",
    ").opts(\n",
    "    hv.opts.Image(frame_width=450, aspect=\"equal\", active_tools=[\"wheel_zoom\"]),\n",
    "    hv.opts.RGB(frame_width=450, aspect=\"equal\", active_tools=[\"wheel_zoom\"]),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build and visualize confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metadata / MoA labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_df = dataset.image_df\n",
    "image_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moa_df = dataset.moa_df\n",
    "moa_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_moas = np.array(dm.all_dataset.moa_df.query('moa != \"null\"')['moa'].unique())\n",
    "all_moas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find subset of MoAs not in test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moa_in_test = moa_df[\"moa\"].unique()\n",
    "\n",
    "missing_moas = set(all_moas).difference(moa_in_test)\n",
    "\n",
    "print(f\"MoAs not in test set: {missing_moas}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct and normalize confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import hvplot.xarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cmat = xr.DataArray(\n",
    "    confusion_matrix(\n",
    "        true_labels,\n",
    "        predicted_labels,\n",
    "        labels=np.arange(\n",
    "            np.array(list(dataset.class_to_label.values())).max() + 1\n",
    "        ),\n",
    "    ),\n",
    "    dims=[\"moa_true\", \"moa_predicted\"],\n",
    "    coords=dict(moa_true=moas, moa_predicted=moas),\n",
    "    name=\"confusion\",\n",
    ")\n",
    "\n",
    "cmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmat_normed = (cmat / cmat.sum(\"moa_predicted\")).pipe(\n",
    "    lambda da: da.where(~da.isnull(), other=0)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmat_normed.hvplot.heatmap(\n",
    "    \"moa_predicted\",\n",
    "    \"moa_true\",\n",
    "    \"confusion\",\n",
    "    rot=45,\n",
    "    frame_width=300,\n",
    "    frame_height=300,\n",
    "    cmap=\"bjy\",\n",
    "    ylabel=\"True MoA\",\n",
    "    xlabel=\"Predicted MoA\",\n",
    "    title=\"Model predictions vs. true MoAs\",\n",
    "    clim=(0, 1),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dump results to be used later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "\n",
    "with open(\"eval_results.pkl\", \"wb\") as fhdl:\n",
    "    pkl.dump(\n",
    "        dict(\n",
    "            model_name=model.__class__.__name__,\n",
    "            mcc=mcc,\n",
    "            cmat=cmat,\n",
    "            cmat_normed=cmat_normed,\n",
    "            image_df=image_df,\n",
    "            moa_df=moa_df,\n",
    "            true_labels=true_labels,\n",
    "            predicted_labels=predicted_labels,\n",
    "            image_idcs=image_idcs,\n",
    "            error_image_idcs=error_image_idcs,\n",
    "            error_predictions=error_predictions,\n",
    "            error_classes=[\n",
    "                dataset.label_to_class[error_predictions[image_idx]]\n",
    "                for image_idx in range(len(error_predictions))\n",
    "            ],\n",
    "        ), fhdl\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
