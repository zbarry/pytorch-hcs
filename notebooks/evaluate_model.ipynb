{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the performance of the model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import holoviews as hv\n",
    "import hvplot.pandas\n",
    "import hvplot.xarray\n",
    "import janitor\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import xarray as xr\n",
    "from pytorch_hcs.datasets import BBBC021DataModule\n",
    "from pytorch_hcs.models import ResNet18, ResNet101, ResNet18Embeddings\n",
    "from pytorch_hcs.vis import set_hv_defaults\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# from pyprojroot import here\n",
    "\n",
    "\n",
    "set_hv_defaults()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path(\"data\")  # here() / \"data\"\n",
    "data_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choose GPU or CPU processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\"\n",
    "# DEVICE = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specify model to load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login()\n",
    "run = wandb.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_artifact_ids = {\n",
    "    \"resnet18\": \"model-1eyyjpad:v0\",\n",
    "    \"resnet18-moreaug\": \"model-3d5kdlrp:v0\",\n",
    "    \"resnet18-notpretrained\": \"model-6bsy7dth:v0\",\n",
    "    \"resnet101\": \"model-3fizb084:v0\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {}\n",
    "\n",
    "for run_name, artifact_id in model_artifact_ids.items():\n",
    "    artifact = run.use_artifact(\n",
    "        f\"zbarry/pytorch-hcs/{model_artifact_ids[run_name]}\", type=\"model\"\n",
    "    )\n",
    "\n",
    "    artifact_dir = artifact.download()\n",
    "\n",
    "    ckpt_path = f\"{artifact_dir}/model.ckpt\"\n",
    "\n",
    "    if \"resnet18\" in run_name:\n",
    "        model_cls = ResNet18\n",
    "    elif \"resnet101\" in run_name:\n",
    "        model_cls = ResNet101\n",
    "    else:\n",
    "        print(\"Could not parse model from run name:\", run_name)\n",
    "\n",
    "        continue\n",
    "\n",
    "    models[run_name] = (\n",
    "        model_cls.load_from_checkpoint(str(ckpt_path)).eval().to(DEVICE)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "#     \"round1\": ResNet18Embeddings.load_from_checkpoint(\n",
    "#         \"/home/zachary/projects/pytorch-hcs/notebooks/data/weights/ResNet18Embeddings/version_3hxtgk45/epoch=22-step=5703.ckpt\"\n",
    "#     )\n",
    "#     .eval()\n",
    "#     .to(DEVICE),\n",
    "    \"round2\": ResNet18Embeddings.load_from_checkpoint(\n",
    "        \"/home/zachary/projects/pytorch-hcs/notebooks/data/weights/ResNet18Embeddings/version_358i4dhs/epoch=29-step=7439.ckpt\"\n",
    "    )\n",
    "    .eval()\n",
    "    .to(DEVICE),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_path = data_path / f\"weights/{run_name}/{model_version}\"\n",
    "\n",
    "# checkpoint_files = list(model_path.glob(\"epoch=*.ckpt\"))\n",
    "\n",
    "# if len(checkpoint_files) > 1:\n",
    "#     raise Exception(\"Too many checkpoint files\")\n",
    "# if len(checkpoint_files) == 0:\n",
    "#     raise FileNotFoundError(\"No checkpoint file exists.\")\n",
    "\n",
    "# checkpoint_file = checkpoint_files[0]\n",
    "\n",
    "# print(checkpoint_file)\n",
    "\n",
    "# model = model_cls.load_from_checkpoint(str(checkpoint_file)).eval().to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up `LightningDataModule`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = BBBC021DataModule(\n",
    "    num_workers=8,\n",
    "    tv_batch_size=16,\n",
    "    t_batch_size=16,\n",
    ")\n",
    "\n",
    "dm.setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterate through test set, extracting predicted class labels from model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = dm.train_dataset\n",
    "# dataloader = dm.train_dataloader()\n",
    "\n",
    "# dataset = dm.val_dataset\n",
    "# dataloader = dm.val_dataloader()\n",
    "\n",
    "dataset = dm.test_dataset\n",
    "dataloader = dm.test_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dfs = []\n",
    "\n",
    "for run_name, model in models.items():\n",
    "\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "    image_idcs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for image_batch, true_labels_batch, metadata_batch in tqdm(dataloader):\n",
    "            image_idcs.extend(metadata_batch.image_idx.numpy())\n",
    "            outputs = model(image_batch.to(DEVICE))\n",
    "\n",
    "            labels = torch.argmax(outputs, 1).cpu()\n",
    "\n",
    "            predicted_labels.extend(labels)\n",
    "\n",
    "            true_labels.extend(true_labels_batch)\n",
    "\n",
    "    true_labels = np.array(true_labels)\n",
    "    predicted_labels = np.array(predicted_labels)\n",
    "    image_idcs = np.array(image_idcs)\n",
    "    \n",
    "    df = pd.DataFrame(dict(run_name=run_name, true_label=true_labels, predicted_label=predicted_labels, image_idx=image_idcs))\n",
    "    \n",
    "    label_dfs.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_df = pd.concat(label_dfs, ignore_index=True).astype(dict(run_name='category'))\n",
    "label_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import matthews_corrcoef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcc_df = (\n",
    "    label_df.groupby(\"run_name\")\n",
    "    .apply(\n",
    "        lambda df: matthews_corrcoef(df[\"true_label\"], df[\"predicted_label\"])\n",
    "    )\n",
    "    .to_frame(\"mcc_score\")\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "mcc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels, predicted_labels, image_idcs =label_df.query('run_name == \"round2\"')[\n",
    "    [\"true_label\", \"predicted_label\", \"image_idx\"]\n",
    "].values.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_image_idcs = image_idcs[np.flatnonzero(predicted_labels != true_labels)]\n",
    "error_image_idcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pybbbc import BBBC021\n",
    "\n",
    "bbbc021 = BBBC021()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_layout(image_idx):\n",
    "    image, metadata = bbbc021[error_image_idcs[image_idx]]\n",
    "\n",
    "    #     prefix = f\"{metadata.compound.compound} @ {metadata.compound.concentration:.2e} Î¼M, {metadata.compound.moa}\"\n",
    "\n",
    "    prefix = f\"{metadata.compound.compound}, {metadata.compound.moa}, {error_image_idcs[image_idx]}\"\n",
    "\n",
    "    plots = []\n",
    "\n",
    "    cmaps = [\"fire\", \"kg\", \"kb\"]\n",
    "\n",
    "    for channel_idx, im_channel in enumerate(image):\n",
    "        plot = hv.Image(\n",
    "            im_channel,\n",
    "            bounds=(0, 0, im_channel.shape[1], im_channel.shape[0]),\n",
    "            label=f\"{prefix} | {bbbc021.CHANNELS[channel_idx]}\",\n",
    "        ).opts(cmap=cmaps[channel_idx])\n",
    "        plots.append(plot)\n",
    "\n",
    "    plots.append(\n",
    "        hv.RGB(\n",
    "            image.transpose(1, 2, 0),\n",
    "            bounds=(0, 0, im_channel.shape[1], im_channel.shape[0]),\n",
    "            label=\"Channel overlay\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return hv.Layout(plots).cols(2)\n",
    "\n",
    "\n",
    "hv.DynamicMap(make_layout, kdims=\"image\").redim.range(\n",
    "    image=(0, len(error_image_idcs) - 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_df = dataset.image_df\n",
    "image_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build list of MoAs for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moa_df = dataset.moa_df\n",
    "moa_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moas = np.array(moa_df[\"moa\"].unique())\n",
    "moas = moas[moas != \"null\"]\n",
    "moas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find subset of MoAs not in test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moa_in_test = image_df[\"moa\"].unique()\n",
    "\n",
    "missing_moas = set(moas).difference(moa_in_test)\n",
    "\n",
    "print(f\"MoAs not in test set: {missing_moas}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate and normalize confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cmat = xr.DataArray(\n",
    "    confusion_matrix(\n",
    "        true_labels,\n",
    "        predicted_labels,\n",
    "        labels=np.arange(\n",
    "            np.array(list(dataset.class_to_label.values())).max() + 1\n",
    "        ),\n",
    "    ),\n",
    "    dims=[\"moa_true\", \"moa_predicted\"],\n",
    "    coords=dict(moa_true=moas, moa_predicted=moas),\n",
    "    name=\"confusion\",\n",
    ")\n",
    "\n",
    "cmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmat_normed = (cmat / cmat.sum(\"moa_predicted\")).pipe(\n",
    "    lambda da: da.where(~da.isnull(), other=0)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmat_normed.hvplot.heatmap(\n",
    "    \"moa_predicted\",\n",
    "    \"moa_true\",\n",
    "    \"confusion\",\n",
    "    rot=45,\n",
    "    frame_width=300,\n",
    "    frame_height=300,\n",
    "    cmap=\"bjy\",\n",
    "    ylabel=\"True MoA\",\n",
    "    xlabel=\"Predicted MoA\",\n",
    "    title=\"Model predictions vs. true MoAs\",\n",
    "    clim=(0, 1),\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-hcs",
   "language": "python",
   "name": "pytorch-hcs"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
